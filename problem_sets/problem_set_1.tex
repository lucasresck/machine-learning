\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Problem set 1\\
    \large{Machine Learning}}
\author{Lucas Emanuel Resck Domingues\\    
    Professor: Rodrigo Targino\\\\
    {School of Applied Mathematics}\\
    {Getulio Vargas Foundation}}
\date{\today}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{fullpage}
\usepackage{enumitem}

\begin{document}

    \maketitle

    \noindent \textbf{\textit{Exercise 1.3}}

    \noindent \textit{The weight update rule in (1.3)~\cite{yaser2012learning} has the nice interpretation that it moves in the direction of classifying $x(t)$ correctly.}
    
    \begin{itemize}[before=\itshape]
        \item[(a)] Show that $y(t) \mathbf{w}^\intercal(t) \mathbf{x}(t) < 0$. [Hint: $\mathbf{x}(t)$ is misclassified by $\mathbf{w}(t)$.]
        \item[(b)] Show that $y(t) \mathbf{w}^\intercal(t + 1) \mathbf{x}(t) > y(t) \mathbf{w}^\intercal(t) \mathbf{x}(t)$. [Hint: Use (1.3)~\cite{yaser2012learning}.]
        \item[(c)] As far as classifying $\mathbf{x(t)}$ is concerned, argue that the move from $\mathbf{w}(t)$ to $\mathbf{w}(t + 1)$ is a move `in the right direction'.
    \end{itemize}
        

    \nocite{yaser2012learning}
    \bibliographystyle{plain}
    \bibliography{references}

\end{document}